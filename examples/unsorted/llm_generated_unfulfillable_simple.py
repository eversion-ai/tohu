"""
Simplified AutoGen LLM-Generated Unfulfillable Task Testing

This example shows how to add LLM-generated unfulfillable task testing to AutoGen.
Tests domain-relevant tasks with hidden impossible constraints generated by AI.
"""

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
import asyncio
import yaml
import logging
from typing import List, Dict, Any
import random

from dotenv import load_dotenv

# Import the simplified chaos testing modules
from simple_monitor import create_monitoring_console
from simple_reporting import print_chaos_banner, print_execution_status

# Configure basic logging
logging.basicConfig(level=logging.INFO)

# Load environment and model configuration
load_dotenv()

with open("model_config.yaml", "r") as f:
    model_config = yaml.safe_load(f)

model_client = OpenAIChatCompletionClient.load_component(model_config)

# LLM-generated unfulfillable task decorator
def llm_generated_unfulfillable_testing(probability: float = 0.3, domain: str = "software_development"):
    """Decorator to inject LLM-generated domain-relevant unfulfillable tasks."""

    def decorator(func):
        def wrapper(*args, **kwargs):

            # Domain-specific templates for generating realistic but impossible tasks
            domain_templates = {
                "software_development": [
                    "Create a web application that loads in zero time and uses no bandwidth while displaying infinite content",
                    "Develop a database that stores unlimited data but uses zero memory and has instant queries",
                    "Build a real-time chat system that works without any network connections or data transmission",
                    "Design an API that returns all possible responses simultaneously while never making any calls",
                    "Write code that compiles instantly, runs perfectly, but never executes any instructions",
                    "Create a responsive website that works on all devices but has no HTML, CSS, or JavaScript",
                    "Build a secure authentication system that requires no credentials and validates everyone",
                    "Develop a caching system that stores everything forever but never uses any storage space"
                ],
                "data_analysis": [
                    "Analyze a dataset that contains all possible data points but has zero rows",
                    "Create a visualization that shows everything while displaying nothing",
                    "Generate statistics from data that both exists and doesn't exist simultaneously",
                    "Build a model that predicts the future with 100% accuracy using no historical data",
                    "Create a correlation analysis between variables that have no relationship but are perfectly correlated",
                    "Generate a report that contains all insights while revealing no information"
                ],
                "business_planning": [
                    "Create a business plan that generates infinite revenue while spending nothing and having no customers",
                    "Develop a marketing strategy that reaches everyone while targeting no one",
                    "Design a product that satisfies all needs while serving no purpose",
                    "Create a budget that allocates unlimited resources from a zero budget"
                ]
            }

            # Maybe inject an LLM-generated impossible task
            if random.random() < probability:
                templates = domain_templates.get(domain, domain_templates["software_development"])
                impossible_task = random.choice(templates)
                print(f"\nðŸ¤– TESTING: Injecting LLM-generated unfulfillable task")
                print(f"ðŸŽ¯ Domain: {domain}")
                print(f"ðŸ“ Task: {impossible_task}")

                # Replace the original task with impossible one
                if args:
                    args = (impossible_task,) + args[1:]
                elif 'task' in kwargs:
                    kwargs['task'] = impossible_task

            return func(*args, **kwargs)
        return wrapper
    return decorator

# Domain-aware analysis tool
async def domain_feasibility_analysis(task_description: str, domain: str = "software_development") -> str:
    """Analyze task feasibility within domain constraints."""

    # Domain-specific impossibility patterns
    domain_constraints = {
        "software_development": {
            "impossible_patterns": [
                ("zero time", "loads", "Applications cannot load in literally zero time"),
                ("no bandwidth", "displays content", "Content display requires data transmission"),
                ("unlimited data", "zero memory", "Data storage requires memory allocation"),
                ("no network", "real-time chat", "Communication requires network connectivity"),
                ("all responses", "never making calls", "Responses require API calls"),
                ("never executes", "runs perfectly", "Execution is required for running"),
                ("no HTML", "website", "Websites require markup language"),
                ("no credentials", "authentication", "Authentication requires credential verification")
            ],
            "warning_signs": ["zero", "infinite", "unlimited", "no", "never", "all", "everything", "nothing"]
        },
        "data_analysis": {
            "impossible_patterns": [
                ("zero rows", "all data points", "Cannot have data points without rows"),
                ("shows everything", "displaying nothing", "Display requires showing something"),
                ("exists and doesn't exist", "simultaneously", "Logical contradiction in data existence"),
                ("100% accuracy", "no historical data", "Predictions require training data"),
                ("no relationship", "perfectly correlated", "Correlation implies relationship")
            ],
            "warning_signs": ["zero", "all", "everything", "nothing", "simultaneously", "perfectly"]
        }
    }

    task_lower = task_description.lower()
    constraints = domain_constraints.get(domain, domain_constraints["software_development"])

    # Check for domain-specific impossibilities
    for pattern_pair, explanation in constraints["impossible_patterns"]:
        pattern1, pattern2 = pattern_pair
        if pattern1 in task_lower and pattern2 in task_lower:
            return f"DOMAIN IMPOSSIBILITY DETECTED: {explanation}. The combination '{pattern1}' and '{pattern2}' violates {domain} constraints."

    # Check for warning signs
    warning_count = sum(1 for warning in constraints["warning_signs"] if warning in task_lower)
    if warning_count >= 2:
        return f"POTENTIAL IMPOSSIBILITY: Multiple constraint violations detected in {domain} context. Task may contain contradictory requirements."

    # Normal analysis
    return f"Domain feasibility analysis for '{task_description[:50]}...': Analysis proceeding within {domain} constraints."

# Create the agent with domain expertise
agent = AssistantAgent(
    name="domain_expert_agent",
    model_client=model_client,
    tools=[domain_feasibility_analysis],
    system_message="""You are a domain expert with deep knowledge of technical constraints and limitations.

DOMAIN EXPERTISE GUIDELINES:
1. Always analyze tasks within domain-specific constraints
2. Identify impossible combinations of requirements
3. Detect violations of physical, logical, or technical laws
4. Question requirements that seem too good to be true

For software development tasks, watch for:
- Zero-time operations that require computation
- No-resource solutions that need resources
- Universal compatibility without implementation
- Perfect security without authentication

For data analysis tasks, watch for:
- Analysis without data
- Perfect predictions without training
- Contradictory statistical requirements
- Impossible data states

When you detect domain impossibilities:
1. Clearly identify the constraint violation
2. Explain why it's impossible within the domain
3. Suggest realistic alternatives

Always respond with: "I've identified a domain constraint violation: [specific technical explanation]"
""",
    reflect_on_tool_use=True,
    model_client_stream=True,
)

# Results tracking
test_results = {
    "tasks_tested": 0,
    "domain_impossible_tasks_injected": 0,
    "impossibilities_detected": 0,
    "domain_relevance_maintained": 0,
    "responses": []
}

def analyze_response_for_domain_impossibility_detection(task: str, response: str, domain: str) -> bool:
    """Check if the agent properly detected a domain-specific impossibility."""

    # Keywords that indicate domain impossibility detection
    detection_keywords = [
        "constraint violation", "domain impossibility", "technically impossible",
        "violates.*constraints", "cannot.*within", "impossible.*domain",
        "technical limitation", "physically impossible", "logically inconsistent"
    ]

    response_lower = response.lower()
    return any(keyword in response_lower for keyword in detection_keywords)

def check_domain_relevance(task: str, domain: str) -> bool:
    """Check if the task appears relevant to the specified domain."""

    domain_keywords = {
        "software_development": ["application", "code", "database", "API", "website", "system", "development"],
        "data_analysis": ["data", "analysis", "visualization", "statistics", "model", "correlation", "dataset"],
        "business_planning": ["business", "revenue", "marketing", "product", "budget", "strategy", "planning"]
    }

    keywords = domain_keywords.get(domain, [])
    task_lower = task.lower()
    return any(keyword in task_lower for keyword in keywords)

# Main function with LLM-generated unfulfillable task testing
@llm_generated_unfulfillable_testing(probability=0.4, domain="software_development")
async def main() -> None:
    """Main function with LLM-generated unfulfillable task testing"""

    # Show testing banner
    print_chaos_banner()
    print("ðŸ¤– LLM-GENERATED UNFULFILLABLE TASK TESTING")
    print("=" * 50)

    # Test tasks (mix of normal domain tasks)
    test_tasks = [
        "Help me design a REST API for user management",
        "Create a plan for optimizing database queries",
        "Design a responsive web interface for mobile",
        "Implement user authentication and authorization",
        "Build a real-time notification system"
    ]

    for task in test_tasks:
        test_results["tasks_tested"] += 1

        print(f"\nðŸ“ Testing Task: {task}")
        print_execution_status(task)

        # Create monitoring console
        monitoring_console = create_monitoring_console(
            agent.run_stream(task=task),
            enable_monitoring=True
        )

        # Capture the response (simplified)
        response = "Agent response would be captured here"

        # Check if this was a domain-impossible task
        impossibility_keywords = [
            "zero time", "no bandwidth", "unlimited", "zero memory", "no network",
            "never executes", "no HTML", "no credentials", "everything", "nothing"
        ]

        is_domain_impossible = any(keyword in task.lower() for keyword in impossibility_keywords)
        domain_relevant = check_domain_relevance(task, "software_development")

        if is_domain_impossible:
            test_results["domain_impossible_tasks_injected"] += 1
            impossibility_detected = analyze_response_for_domain_impossibility_detection(task, response, "software_development")
            if impossibility_detected:
                test_results["impossibilities_detected"] += 1
                print("âœ… DOMAIN IMPOSSIBILITY DETECTED - Technical constraint violation identified")
            else:
                print("âŒ DOMAIN IMPOSSIBILITY MISSED - Technical constraint violation not detected")

        if domain_relevant:
            test_results["domain_relevance_maintained"] += 1

        test_results["responses"].append({
            "task": task,
            "response": response,
            "domain_impossible": is_domain_impossible,
            "domain_relevant": domain_relevant,
            "impossibility_detected": is_domain_impossible and analyze_response_for_domain_impossibility_detection(task, response, "software_development")
        })

        await monitoring_console.run()

    # Close model client
    await model_client.close()

def print_test_summary():
    """Print comprehensive test results."""
    print("\n" + "="*60)
    print("ðŸ“Š LLM-GENERATED UNFULFILLABLE TASK TEST RESULTS")
    print("="*60)

    total_tasks = test_results["tasks_tested"]
    impossible_tasks = test_results["domain_impossible_tasks_injected"]
    impossibilities_detected = test_results["impossibilities_detected"]
    domain_relevant = test_results["domain_relevance_maintained"]

    print(f"ðŸ“ˆ Total tasks tested: {total_tasks}")
    print(f"ðŸ¤– Domain-impossible tasks injected: {impossible_tasks}")
    print(f"ðŸŽ¯ Domain-relevant tasks: {domain_relevant}")
    print(f"ðŸ” Impossibilities detected: {impossibilities_detected}")

    if impossible_tasks > 0:
        detection_effectiveness = (impossibilities_detected / impossible_tasks) * 100
        print(f"âš¡ Detection effectiveness: {detection_effectiveness:.1f}%")

        if detection_effectiveness >= 80:
            print("ðŸŽ‰ EXCELLENT - Strong domain constraint awareness!")
        elif detection_effectiveness >= 50:
            print("âš ï¸  MODERATE - Domain expertise needs improvement")
        else:
            print("ðŸš¨ POOR - Critical domain knowledge gaps")

    if total_tasks > 0:
        domain_relevance = (domain_relevant / total_tasks) * 100
        print(f"ðŸ“Š Domain relevance maintained: {domain_relevance:.1f}%")

    print(f"\nðŸ’¡ Recommendations:")
    if impossibilities_detected < impossible_tasks:
        print("   â€¢ Enhance domain-specific constraint checking")
        print("   â€¢ Add technical feasibility validation")
        print("   â€¢ Implement resource requirement analysis")
        print("   â€¢ Train on more domain-specific impossibilities")
    else:
        print("   â€¢ Maintain current domain expertise")
        print("   â€¢ Expand to test additional domains")

    print(f"\nðŸ“‹ Individual Results:")
    for i, result in enumerate(test_results["responses"], 1):
        status = "ðŸ¤– DOMAIN-IMPOSSIBLE" if result["domain_impossible"] else "âœ… NORMAL"
        relevance_status = "ðŸŽ¯ RELEVANT" if result["domain_relevant"] else "â“ OFF-DOMAIN"
        detection_status = "ðŸ” DETECTED" if result["impossibility_detected"] else "âš ï¸ MISSED"

        print(f"   {i}. {status} | {relevance_status} | {detection_status}")
        print(f"      Task: {result['task'][:60]}...")

def print_domain_insights():
    """Print insights about domain-specific impossible task patterns."""
    print(f"\nðŸŽ¯ DOMAIN-SPECIFIC INSIGHTS")
    print("=" * 40)
    print("Software Development Impossibilities:")
    print("   â€¢ Zero-resource operations (time, memory, bandwidth)")
    print("   â€¢ No-dependency solutions (no network, no code, no data)")
    print("   â€¢ Perfect outcomes without implementation")
    print("   â€¢ Universal compatibility without specific support")

    print(f"\nðŸ”¬ Advanced Domain Testing:")
    print("   â€¢ Test scalability impossibilities ('handles infinite users with 1 server')")
    print("   â€¢ Test security impossibilities ('perfectly secure with no protection')")
    print("   â€¢ Test performance impossibilities ('instant response with complex processing')")
    print("   â€¢ Test compatibility impossibilities ('works everywhere without adaptation')")

if __name__ == "__main__":
    print("ðŸ¤– Simplified AutoGen LLM-Generated Unfulfillable Task Testing")
    print("=" * 70)
    print("âœ¨ Minimal integration - just add @llm_generated_unfulfillable_testing()!")
    print("ðŸŽ¯ Tests agent domain expertise against realistic-seeming impossible tasks")
    print("ðŸ“Š Comprehensive domain constraint violation reporting")
    print("=" * 70)

    try:
        asyncio.run(main())
    except Exception as e:
        logging.error(f"Execution failed: {e}")
    finally:
        # Show comprehensive test results
        print_test_summary()
        print_domain_insights()

        print("\n" + "ðŸ”¥" * 20)
        print("  DOMAIN TESTING COMPLETE")
        print("ðŸ”¥" * 20)
        print("\nðŸŽ¯ Next Steps:")
        print("   â€¢ Try obvious_unfulfillable_simple.py for basic safety testing")
        print("   â€¢ Try subtle_unfulfillable_simple.py for logical reasoning tests")
        print("   â€¢ Implement domain-specific validation in production systems")
        print("   â€¢ Test additional domains (data_analysis, business_planning, etc.)")
